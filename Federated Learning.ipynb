{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30700,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#===========================================================\n# Federated learning: ResNet18 on HAM10000\n# HAM10000 dataset: Tschandl, P.: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions (2018), doi:10.7910/DVN/DBW86T\n\n# We have three versions of our implementations\n# Version1: without using socket and no DP+PixelDP\n# Version2: with using socket but no DP+PixelDP\n# Version3: without using socket but with DP+PixelDP\n\n# This program is Version1: Single program simulation \n# ===========================================================\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom pandas import DataFrame\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom glob import glob \nimport math\nimport random\nimport numpy as np\nimport os\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport copy\n\n\nSEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n    print(torch.cuda.get_device_name(0))    \n\n\n#===================================================================  \nprogram = \"FL ResNet18 on HAM10000\"\nprint(f\"---------{program}----------\")              # this is to identify the program in the slurm outputs files\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# To print in color during test/train \ndef prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \ndef prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))    \n\n\n#===================================================================\n# No. of users\nnum_users = 5\nepochs = 10\nfrac = 1\nlr = 0.0001\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:53.996919Z","iopub.execute_input":"2024-05-03T22:43:53.997886Z","iopub.status.idle":"2024-05-03T22:43:58.060136Z","shell.execute_reply.started":"2024-05-03T22:43:53.997837Z","shell.execute_reply":"2024-05-03T22:43:58.059187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#==============================================================================================================\n#                                  Client Side Program \n#==============================================================================================================\nclass DatasetSplit(Dataset):\n    def __init__(self, dataset, idxs):\n        self.dataset = dataset\n        self.idxs = list(idxs)\n\n    def __len__(self):\n        return len(self.idxs)\n\n    def __getitem__(self, item):\n        image, label = self.dataset[self.idxs[item]]\n        return image, label\n\n# Client-side functions associated with Training and Testing\nclass LocalUpdate(object):\n    def __init__(self, idx, lr, device, dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n        self.idx = idx\n        self.device = device\n        self.lr = lr\n        self.local_ep = 1\n        self.loss_func = nn.CrossEntropyLoss()\n        self.selected_clients = []\n        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = 256, shuffle = True)\n        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = 256, shuffle = True)\n\n    def train(self, net):\n        net.train()\n        # train and update\n        #optimizer = torch.optim.SGD(net.parameters(), lr = self.lr, momentum = 0.5)\n        optimizer = torch.optim.Adam(net.parameters(), lr = self.lr)\n        \n        epoch_acc = []\n        epoch_loss = []\n        for iter in range(self.local_ep):\n            batch_acc = []\n            batch_loss = []\n            \n            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n                images, labels = images.to(self.device), labels.to(self.device)\n                optimizer.zero_grad()\n                #---------forward prop-------------\n                fx = net(images)\n                \n                # calculate loss\n                loss = self.loss_func(fx, labels)\n                # calculate accuracy\n                acc = calculate_accuracy(fx, labels)\n                \n                #--------backward prop--------------\n                loss.backward()\n                optimizer.step()\n                              \n                batch_loss.append(loss.item())\n                batch_acc.append(acc.item())\n            \n            \n            prRed('Client{} Train => Local Epoch: {}  \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(self.idx,\n                        iter, acc.item(), loss.item()))\n            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n            epoch_acc.append(sum(batch_acc)/len(batch_acc))\n        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc)\n    \n    def evaluate(self, net):\n        net.eval()\n           \n        epoch_acc = []\n        epoch_loss = []\n        with torch.no_grad():\n            batch_acc = []\n            batch_loss = []\n            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n                images, labels = images.to(self.device), labels.to(self.device)\n                #---------forward prop-------------\n                fx = net(images)\n                \n                # calculate loss\n                loss = self.loss_func(fx, labels)\n                # calculate accuracy\n                acc = calculate_accuracy(fx, labels)\n                                 \n                batch_loss.append(loss.item())\n                batch_acc.append(acc.item())\n            \n            prGreen('Client{} Test =>                     \\tLoss: {:.4f} \\tAcc: {:.3f}'.format(self.idx, loss.item(), acc.item()))\n            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n            epoch_acc.append(sum(batch_acc)/len(batch_acc))\n        return sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.062054Z","iopub.execute_input":"2024-05-03T22:43:58.062498Z","iopub.status.idle":"2024-05-03T22:43:58.081597Z","shell.execute_reply.started":"2024-05-03T22:43:58.062472Z","shell.execute_reply":"2024-05-03T22:43:58.080608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=============================================================================\n#                         Data loading \n#============================================================================= \ndf = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\nprint(df.head())\n\nlesion_type = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n\n# merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\nimageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n                for x in glob(os.path.join(\"/kaggle/input/skin-cancer-mnist-ham10000\", '*', '*.jpg'))}\n\n\n#print(\"path---------------------------------------\", imageid_path.get)\ndf['path'] = df['image_id'].map(imageid_path.get)\ndf['cell_type'] = df['dx'].map(lesion_type.get)\ndf['target'] = pd.Categorical(df['cell_type']).codes\nprint(df['cell_type'].value_counts())\nprint(df['target'].value_counts())\n# print(imageid_path)\n# print(df['path'])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.082692Z","iopub.execute_input":"2024-05-03T22:43:58.082962Z","iopub.status.idle":"2024-05-03T22:43:58.274175Z","shell.execute_reply.started":"2024-05-03T22:43:58.082938Z","shell.execute_reply":"2024-05-03T22:43:58.273292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#==============================================================\n# Custom dataset prepration in Pytorch format\nclass SkinData(Dataset):\n    def __init__(self, df, transform = None):\n        \n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        \n        return len(self.df)\n    \n    def __getitem__(self, index):\n        \n        X = Image.open(self.df['path'][index]).resize((64, 64))\n        y = torch.tensor(int(self.df['target'][index]))\n        \n        if self.transform:\n            X = self.transform(X)\n        \n        return X, y","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.276463Z","iopub.execute_input":"2024-05-03T22:43:58.276755Z","iopub.status.idle":"2024-05-03T22:43:58.283833Z","shell.execute_reply.started":"2024-05-03T22:43:58.276730Z","shell.execute_reply":"2024-05-03T22:43:58.282752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=====================================================================================================\n# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n# IID HAM10000 datasets will be created based on this\ndef dataset_iid(dataset, num_users):\n    \n    num_items = int(len(dataset)/num_users)\n    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n        all_idxs = list(set(all_idxs) - dict_users[i])\n    return dict_users    ","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.285073Z","iopub.execute_input":"2024-05-03T22:43:58.285365Z","iopub.status.idle":"2024-05-03T22:43:58.293949Z","shell.execute_reply.started":"2024-05-03T22:43:58.285339Z","shell.execute_reply":"2024-05-03T22:43:58.292978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=============================================================================\n# Train-test split  \ntrain, test = train_test_split(df, test_size = 0.2)\n\ntrain = train.reset_index()\ntest = test.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.294930Z","iopub.execute_input":"2024-05-03T22:43:58.295197Z","iopub.status.idle":"2024-05-03T22:43:58.314072Z","shell.execute_reply.started":"2024-05-03T22:43:58.295173Z","shell.execute_reply":"2024-05-03T22:43:58.313087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=============================================================================\n#                         Data preprocessing\n#=============================================================================  \n# Data preprocessing: Transformation \nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([transforms.RandomHorizontalFlip(), \n                        transforms.RandomVerticalFlip(),\n                        transforms.Pad(3),\n                        transforms.RandomRotation(10),\n                        transforms.CenterCrop(64),\n                        transforms.ToTensor(), \n                        transforms.Normalize(mean = mean, std = std)\n                        ])\n    \ntest_transforms = transforms.Compose([\n                        transforms.Pad(3),\n                        transforms.CenterCrop(64),\n                        transforms.ToTensor(), \n                        transforms.Normalize(mean = mean, std = std)\n                        ])    \n\n# With augmentation\ndataset_train = SkinData(train, transform = train_transforms)\ndataset_test = SkinData(test, transform = test_transforms)\n \n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.315379Z","iopub.execute_input":"2024-05-03T22:43:58.315747Z","iopub.status.idle":"2024-05-03T22:43:58.324532Z","shell.execute_reply.started":"2024-05-03T22:43:58.315718Z","shell.execute_reply":"2024-05-03T22:43:58.323442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------------------------------------------\ndict_users = dataset_iid(dataset_train, num_users)\ndict_users_test = dataset_iid(dataset_test, num_users)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.325663Z","iopub.execute_input":"2024-05-03T22:43:58.325980Z","iopub.status.idle":"2024-05-03T22:43:58.345891Z","shell.execute_reply.started":"2024-05-03T22:43:58.325953Z","shell.execute_reply":"2024-05-03T22:43:58.344916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#====================================================================================================\n#                               Server Side Program\n#====================================================================================================\ndef calculate_accuracy(fx, y):\n    preds = fx.max(1, keepdim=True)[1]\n    correct = preds.eq(y.view_as(preds)).sum()\n    acc = 100.00 *correct.float()/preds.shape[0]\n    return acc","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.347079Z","iopub.execute_input":"2024-05-03T22:43:58.347426Z","iopub.status.idle":"2024-05-03T22:43:58.357347Z","shell.execute_reply.started":"2024-05-03T22:43:58.347375Z","shell.execute_reply":"2024-05-03T22:43:58.356434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model try2 \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ResNet18(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        super(ResNet18, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Use AdaptiveAvgPool2d to ensure consistent output size\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\nnet_glob = ResNet18(BasicBlock, [2, 2, 2, 2], 7)  # Assuming 7 classes\nif torch.cuda.device_count() > 1:\n    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n    net_glob = nn.DataParallel(net_glob)  # Utilize multiple GPUs if available\n\nnet_glob.to(device)\nprint(net_glob)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.721136Z","iopub.execute_input":"2024-05-03T22:43:58.721442Z","iopub.status.idle":"2024-05-03T22:43:58.967630Z","shell.execute_reply.started":"2024-05-03T22:43:58.721416Z","shell.execute_reply":"2024-05-03T22:43:58.966323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===========================================================================================\n# Federated averaging: FedAvg\ndef FedAvg(w):\n    w_avg = copy.deepcopy(w[0])\n    for k in w_avg.keys():\n        for i in range(1, len(w)):\n            w_avg[k] += w[i][k]\n        w_avg[k] = torch.div(w_avg[k], len(w))\n    return w_avg","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.969188Z","iopub.execute_input":"2024-05-03T22:43:58.969606Z","iopub.status.idle":"2024-05-03T22:43:58.976887Z","shell.execute_reply.started":"2024-05-03T22:43:58.969563Z","shell.execute_reply":"2024-05-03T22:43:58.975874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#====================================================\nnet_glob.train()\n# copy weights\nw_glob = net_glob.state_dict()\n\nloss_train_collect = []\nacc_train_collect = []\nloss_test_collect = []\nacc_test_collect = []\n\nfor iter in range(epochs):\n    w_locals, loss_locals_train, acc_locals_train, loss_locals_test, acc_locals_test = [], [], [], [], []\n    m = max(int(frac * num_users), 1)\n    idxs_users = np.random.choice(range(num_users), m, replace = False)\n    \n    # Training/Testing simulation\n    for idx in idxs_users: # each client\n        local = LocalUpdate(idx, lr, device, dataset_train = dataset_train, dataset_test = dataset_test, idxs = dict_users[idx], idxs_test = dict_users_test[idx])\n        # Training ------------------\n        w, loss_train, acc_train = local.train(net = copy.deepcopy(net_glob).to(device))\n        w_locals.append(copy.deepcopy(w))\n        loss_locals_train.append(copy.deepcopy(loss_train))\n        acc_locals_train.append(copy.deepcopy(acc_train))\n        # Testing -------------------\n        loss_test, acc_test = local.evaluate(net = copy.deepcopy(net_glob).to(device))\n        loss_locals_test.append(copy.deepcopy(loss_test))\n        acc_locals_test.append(copy.deepcopy(acc_test))\n        \n        \n    \n    # Federation process\n    w_glob = FedAvg(w_locals)\n    print(\"------------------------------------------------\")\n    print(\"------ Federation process at Server-Side -------\")\n    print(\"------------------------------------------------\")\n    \n    # update global model --- copy weight to net_glob -- distributed the model to all users\n    net_glob.load_state_dict(w_glob)\n    \n    # Train/Test accuracy\n    acc_avg_train = sum(acc_locals_train) / len(acc_locals_train)\n    acc_train_collect.append(acc_avg_train)\n    acc_avg_test = sum(acc_locals_test) / len(acc_locals_test)\n    acc_test_collect.append(acc_avg_test)\n    \n    # Train/Test loss\n    loss_avg_train = sum(loss_locals_train) / len(loss_locals_train)\n    loss_train_collect.append(loss_avg_train)\n    loss_avg_test = sum(loss_locals_test) / len(loss_locals_test)\n    loss_test_collect.append(loss_avg_test)\n    \n    \n    print('------------------- SERVER ----------------------------------------------')\n    print('Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, acc_avg_train, loss_avg_train))\n    print('Test:  Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(iter, acc_avg_test, loss_avg_test))\n    print('-------------------------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:43:58.978610Z","iopub.execute_input":"2024-05-03T22:43:58.979302Z","iopub.status.idle":"2024-05-03T22:59:04.660861Z","shell.execute_reply.started":"2024-05-03T22:43:58.979266Z","shell.execute_reply":"2024-05-03T22:59:04.659721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===================================================================================     \n\nprint(\"Training and Evaluation completed!\")    \n\n#===============================================================================","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:59:04.663651Z","iopub.execute_input":"2024-05-03T22:59:04.664018Z","iopub.status.idle":"2024-05-03T22:59:04.669560Z","shell.execute_reply.started":"2024-05-03T22:59:04.663985Z","shell.execute_reply":"2024-05-03T22:59:04.668703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save output data to .excel file (we use for comparision plots)\nround_process = [i for i in range(1, len(acc_train_collect)+1)]\ndf = DataFrame({'round': round_process,'acc_train':acc_train_collect, 'acc_test':acc_test_collect})     \nfile_name = program+\".xlsx\"    \ndf.to_excel(file_name, sheet_name= \"v1_test\", index = False)     \n\n#=============================================================================\n#                         Program Completed\n#============================================================================= \n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:59:04.670860Z","iopub.execute_input":"2024-05-03T22:59:04.671160Z","iopub.status.idle":"2024-05-03T22:59:05.144470Z","shell.execute_reply.started":"2024-05-03T22:59:04.671135Z","shell.execute_reply":"2024-05-03T22:59:05.143705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}